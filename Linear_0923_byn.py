import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def load_preprocessed_data():
    """åŠ è½½é¢„å¤„ç†åçš„æ•°æ®"""
    try:
        df = pd.read_csv('preprocessed_data.csv', encoding='utf-8-sig')
        print("æˆåŠŸåŠ è½½é¢„å¤„ç†æ•°æ®")
        return df
    except FileNotFoundError:
        print("æœªæ‰¾åˆ°é¢„å¤„ç†æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ dataPreprocessing_0926_byn.py")
        return None

def build_linear_models(df):
    """æ„å»ºå¤šç§çº¿æ€§å›å½’æ¨¡å‹"""
    
    # å®šä¹‰ä¸åŒçš„æ¨¡å‹é…ç½®
    model_configs = {
        'model1_original': {
            'X': df[['è’¸å‘é‡', 'é™é›¨é‡']],
            'y': df['å¾„æµé‡'],
            'name': 'åŸå§‹æ•°æ®çº¿æ€§å›å½’',
            'features': ['è’¸å‘é‡', 'é™é›¨é‡']
        },
        'model2_log_transformed': {
            'X': df[['è’¸å‘é‡_std', 'é™é›¨é‡_log']],
            'y': df['å¾„æµé‡_log'],
            'name': 'å¯¹æ•°å˜æ¢çº¿æ€§å›å½’',
            'features': ['è’¸å‘é‡_std', 'é™é›¨é‡_log']
        },
        'model3_rainfall_only': {
            'X': df[['é™é›¨é‡_log']].values.reshape(-1, 1),
            'y': df['å¾„æµé‡_log'],
            'name': 'ä»…é™é›¨é‡çº¿æ€§å›å½’',
            'features': ['é™é›¨é‡_log']
        }
    }
    
    results = {}
    
    for model_key, config in model_configs.items():
        print(f"\n=== {config['name']} ===")
        
        # æ•°æ®å‡†å¤‡
        X = config['X']
        y = config['y']
        
        # åˆ†å‰²è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # æ„å»ºæ¨¡å‹
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        
        # è¯„ä¼°æŒ‡æ ‡
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        train_mae = mean_absolute_error(y_train, y_train_pred)
        test_mae = mean_absolute_error(y_test, y_test_pred)
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
        
        # ä¿å­˜ç»“æœ
        results[model_key] = {
            'model': model,
            'config': config,
            'X_train': X_train, 'X_test': X_test,
            'y_train': y_train, 'y_test': y_test,
            'y_train_pred': y_train_pred, 'y_test_pred': y_test_pred,
            'metrics': {
                'train_r2': train_r2,
                'test_r2': test_r2,
                'train_rmse': train_rmse,
                'test_rmse': test_rmse,
                'train_mae': train_mae,
                'test_mae': test_mae,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
        }
        
        # æ‰“å°ç»“æœ
        print(f"è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
        print(f"æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
        print(f"äº¤å‰éªŒè¯ RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
        print(f"æµ‹è¯•é›† RMSE: {test_rmse:.4f}")
        print(f"æµ‹è¯•é›† MAE: {test_mae:.4f}")
        
        # è¾“å‡ºå›å½’ç³»æ•°
        if len(config['features']) > 1:
            print("å›å½’ç³»æ•°:")
            for feature, coef in zip(config['features'], model.coef_):
                print(f"  {feature}: {coef:.4f}")
        else:
            print(f"å›å½’ç³»æ•°: {model.coef_[0]:.4f}")
        print(f"æˆªè·: {model.intercept_:.4f}")
    
    return results

def visualize_results(results):
    """å¯è§†åŒ–æ¨¡å‹ç»“æœ"""
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    for i, (model_key, result) in enumerate(results.items()):
        # é¢„æµ‹ vs å®é™…å€¼æ•£ç‚¹å›¾
        ax = axes[i]
        
        # è®­ç»ƒé›†
        ax.scatter(result['y_train'], result['y_train_pred'], 
                  alpha=0.6, color='blue', label='è®­ç»ƒé›†', s=30)
        # æµ‹è¯•é›†
        ax.scatter(result['y_test'], result['y_test_pred'], 
                  alpha=0.8, color='red', label='æµ‹è¯•é›†', s=30)
        
        # å®Œç¾é¢„æµ‹çº¿
        min_val = min(result['y_train'].min(), result['y_test'].min())
        max_val = max(result['y_train'].max(), result['y_test'].max())
        ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)
        
        ax.set_xlabel('å®é™…å€¼')
        ax.set_ylabel('é¢„æµ‹å€¼')
        ax.set_title(f"{result['config']['name']}\\nRÂ² = {result['metrics']['test_r2']:.3f}")
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    # éšè—å¤šä½™çš„å­å›¾
    for i in range(len(results), 3):
        if i < 3:
            axes[i].set_visible(False)
    
    plt.tight_layout()
    plt.show()

def compare_models(results):
    """æ¯”è¾ƒæ¨¡å‹æ€§èƒ½"""
    print("\\n" + "="*60)
    print("æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
    print("="*60)
    
    comparison_data = []
    for model_key, result in results.items():
        comparison_data.append({
            'æ¨¡å‹': result['config']['name'],
            'æµ‹è¯•é›†RÂ²': f"{result['metrics']['test_r2']:.4f}",
            'äº¤å‰éªŒè¯RÂ²': f"{result['metrics']['cv_mean']:.4f}Â±{result['metrics']['cv_std']:.4f}",
            'RMSE': f"{result['metrics']['test_rmse']:.4f}",
            'MAE': f"{result['metrics']['test_mae']:.4f}"
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model_key = max(results.keys(), 
                        key=lambda k: results[k]['metrics']['test_r2'])
    best_model = results[best_model_key]
    
    print(f"\\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['config']['name']}")
    print(f"   æµ‹è¯•é›† RÂ²: {best_model['metrics']['test_r2']:.4f}")
    print(f"   äº¤å‰éªŒè¯ RÂ²: {best_model['metrics']['cv_mean']:.4f}")

if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    print("å¼€å§‹çº¿æ€§å›å½’å»ºæ¨¡...")
    df = load_preprocessed_data()
    
    if df is not None:
        # æ„å»ºæ¨¡å‹
        results = build_linear_models(df)
        
        # å¯è§†åŒ–ç»“æœ
        print("\\nç”Ÿæˆæ¨¡å‹ç»“æœå›¾...")
        visualize_results(results)
        
        # æ¯”è¾ƒæ¨¡å‹
        compare_models(results)
        
        print("\\nçº¿æ€§å›å½’å»ºæ¨¡å®Œæˆï¼")
    else:
        print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ï¼")